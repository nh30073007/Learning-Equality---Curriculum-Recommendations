{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ae653ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed content_df:\n",
      "                id                                         title  \\\n",
      "3   c_0000c03adc8d                           Nado de aproxima√ß√£o   \n",
      "4   c_00016694ea2a              geometry-m3-topic-a-overview.pdf   \n",
      "12  c_0005765779c8                         –ï–Ω–¥–æ–º–µ–º–±—Ä–∞–Ω–Ω–∞ —Å–∏—Å—Ç–µ–º–∞   \n",
      "14  c_00068709797c  Kuhesabu vizio mraba kutafuta kanuni ya eneo   \n",
      "17  c_0006f51dc8e7                                    Lecci√≥n 13   \n",
      "\n",
      "                                          description      kind  \\\n",
      "3   neste v√≠deo voc√™ vai aprender nado de aproxima...  document   \n",
      "4                                                      document   \n",
      "12  –ø—Ä–µ–≥–ª–µ–¥ –Ω–∞ –º–µ–º–±—Ä–∞–Ω–Ω–∏—Ç–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏ –∫–æ–∏—Ç–æ –æ–±—Ä–∞–∑—É–≤...     video   \n",
      "14  sal anatumia kizio mraba kuona kwa nini kuzidi...     video   \n",
      "17  objetivo multiplicar factores num√©ricos mixtos...  document   \n",
      "\n",
      "                                                 text language  \\\n",
      "3   nado de aproxima√ß√£o saber nadar na ondas sem p...       pt   \n",
      "4   est√°ndares comunes del estado de nueva york pl...       es   \n",
      "12  –≤ —Ç–æ–≤–∞ –≤–∏–¥–µ–æ —â–µ —Ä–∞–∑–≥–ª–µ–¥–∞–º–µ –µ–Ω–¥–æ–º–µ–º–±—Ä–∞–Ω–Ω–∞—Ç–∞ —Å–∏—Å...       bg   \n",
      "14  nina mistatili mitatu hapa na pia nina vipimo ...       sw   \n",
      "17  lesson 13 5 ny common core mathematics curricu...       es   \n",
      "\n",
      "    copyright_holder      license  \n",
      "3   Sikana Education  CC BY-NC-ND  \n",
      "4          Engage NY  CC BY-NC-SA  \n",
      "12      Khan Academy  CC BY-NC-SA  \n",
      "14      Khan Academy  CC BY-NC-SA  \n",
      "17         Engage NY  CC BY-NC-SA  \n",
      "Preprocessed topics_df:\n",
      "                id                                             title  \\\n",
      "0   t_00004da3a1b2                        –æ—Ç–∫—Ä–∏–≤–∞–Ω–µ—Ç–æ –Ω–∞ —Ä–µ–∑–∏—Å—Ç–æ—Ä–∏—Ç–µ   \n",
      "2   t_00068291e9a4                   entradas e sa√≠das de uma fun√ß√£o   \n",
      "4   t_0006d41a73a8  –≥—Ä–∞—Ñ–∏–∫–∏ –Ω–∞ –µ–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª–Ω–∏ —Ñ—É–Ω–∫—Ü–∏–∏ –∞–ª–≥–µ–±—Ä–∞ 2 –Ω–∏–≤–æ   \n",
      "5   t_0008768bdee6                                               100   \n",
      "10  t_00102869fbcb                                  triangle polygon   \n",
      "\n",
      "                                          description channel      category  \\\n",
      "0   –∏–∑—Å–ª–µ–¥–≤–∞–Ω–µ –Ω–∞ –º–∞—Ç–µ—Ä–∏–∞–ª–∏ –∫–æ–∏—Ç–æ –ø—Ä–µ–¥–∏–∑–≤–∏–∫–≤–∞—Ç –Ω–∞–º...  000cf7        source   \n",
      "2                 entenda um pouco mais sobre fun√ß√µes  8e286a        source   \n",
      "4   –Ω–∞—É—á–∏ –ø–æ–≤–µ—á–µ –∑–∞ –≥—Ä–∞—Ñ–∏–∫–∏—Ç–µ –Ω–∞ —Å–ª–æ–∂–Ω–∏—Ç–µ –ø–æ–∫–∞–∑–∞—Ç–µ...  000cf7        source   \n",
      "5                                               37 49  5223e0  supplemental   \n",
      "10  learning outcome student must able solve probl...  a91e32       aligned   \n",
      "\n",
      "    level language          parent  has_content  \n",
      "0       4       bg  t_16e29365b50d         True  \n",
      "2       4       pt  t_d14b6c2a2b70         True  \n",
      "4       4       bg  t_e2452e21d252         True  \n",
      "5       4       gu  t_0da7a331d666         True  \n",
      "10      3       en  t_039cecc12bb8         True  \n"
     ]
    }
   ],
   "source": [
    "#preprocess task with nlp\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#NLTK RESOURCES\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# DATA PATH\n",
    "content_df = pd.read_csv(r'C:\\Users\\nh013\\Desktop\\Learning Equality - Curriculum Recommendations\\content.csv')\n",
    "correlations_df = pd.read_csv(r'C:\\Users\\nh013\\Desktop\\Learning Equality - Curriculum Recommendations\\correlations.csv')\n",
    "topics_df = pd.read_csv(r'C:\\Users\\nh013\\Desktop\\Learning Equality - Curriculum Recommendations\\topics.csv')\n",
    "\n",
    "# IDENTIFY MISSING VALUES\n",
    "missing_values_content = content_df.isnull().sum()\n",
    "missing_values_correlations = correlations_df.isnull().sum()\n",
    "missing_values_topics = topics_df.isnull().sum()\n",
    "\n",
    "# DROP MISSING VALUES\n",
    "content_df.dropna(inplace=True)\n",
    "correlations_df.dropna(inplace=True)\n",
    "topics_df.dropna(inplace=True)\n",
    "\n",
    "# REMOVE DUPLICATE ROWS\n",
    "content_df.drop_duplicates(inplace=True)\n",
    "correlations_df.drop_duplicates(inplace=True)\n",
    "topics_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# NLP PREPROCESSING FUNCTION\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [token for token in tokens if token.isalnum()]\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        return \" \".join(tokens)\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "# NLP PREPROCESSING IN CONTENT_DF\n",
    "content_df['description'] = content_df['description'].apply(preprocess_text)\n",
    "content_df['text'] = content_df['text'].apply(preprocess_text)\n",
    "\n",
    "#NLP PREPROCESSING IN TOPICS_DF\n",
    "topics_df['title'] = topics_df['title'].apply(preprocess_text)\n",
    "topics_df['description'] = topics_df['description'].apply(preprocess_text)\n",
    "topics_df['channel'] = topics_df['channel'].apply(preprocess_text)\n",
    "topics_df['category'] = topics_df['category'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "print(\"Preprocessed content_df:\")\n",
    "print(content_df.head())\n",
    "\n",
    "print(\"Preprocessed topics_df:\")\n",
    "print(topics_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b36f744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0692c5c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014a5616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8de32b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed content_df:\n",
      "                id                                         title  \\\n",
      "3   c_0000c03adc8d                           Nado de aproxima√ß√£o   \n",
      "4   c_00016694ea2a              geometry-m3-topic-a-overview.pdf   \n",
      "12  c_0005765779c8                         –ï–Ω–¥–æ–º–µ–º–±—Ä–∞–Ω–Ω–∞ —Å–∏—Å—Ç–µ–º–∞   \n",
      "14  c_00068709797c  Kuhesabu vizio mraba kutafuta kanuni ya eneo   \n",
      "17  c_0006f51dc8e7                                    Lecci√≥n 13   \n",
      "\n",
      "                                          description      kind  \\\n",
      "3   neste v√≠deo voc√™ vai aprender nado de aproxima...  document   \n",
      "4                                                      document   \n",
      "12  –ø—Ä–µ–≥–ª–µ–¥ –Ω–∞ –º–µ–º–±—Ä–∞–Ω–Ω–∏—Ç–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏ –∫–æ–∏—Ç–æ –æ–±—Ä–∞–∑—É–≤...     video   \n",
      "14  sal anatumia kizio mraba kuona kwa nini kuzidi...     video   \n",
      "17  objetivo multiplicar factores num√©ricos mixtos...  document   \n",
      "\n",
      "                                                 text language  \\\n",
      "3   nado de aproxima√ß√£o saber nadar na ondas sem p...       pt   \n",
      "4   est√°ndares comunes del estado de nueva york pl...       es   \n",
      "12  –≤ —Ç–æ–≤–∞ –≤–∏–¥–µ–æ —â–µ —Ä–∞–∑–≥–ª–µ–¥–∞–º–µ –µ–Ω–¥–æ–º–µ–º–±—Ä–∞–Ω–Ω–∞—Ç–∞ —Å–∏—Å...       bg   \n",
      "14  nina mistatili mitatu hapa na pia nina vipimo ...       sw   \n",
      "17  lesson 13 5 ny common core mathematics curricu...       es   \n",
      "\n",
      "    copyright_holder      license  \\\n",
      "3   Sikana Education  CC BY-NC-ND   \n",
      "4          Engage NY  CC BY-NC-SA   \n",
      "12      Khan Academy  CC BY-NC-SA   \n",
      "14      Khan Academy  CC BY-NC-SA   \n",
      "17         Engage NY  CC BY-NC-SA   \n",
      "\n",
      "                                        combined_text        10  ...  —Ç–æ–π  \\\n",
      "3   neste v√≠deo voc√™ vai aprender nado de aproxima...  0.000000  ...  0.0   \n",
      "4    est√°ndares comunes del estado de nueva york p...  0.040026  ...  0.0   \n",
      "12  –ø—Ä–µ–≥–ª–µ–¥ –Ω–∞ –º–µ–º–±—Ä–∞–Ω–Ω–∏—Ç–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏ –∫–æ–∏—Ç–æ –æ–±—Ä–∞–∑—É–≤...  0.000000  ...  0.0   \n",
      "14  sal anatumia kizio mraba kuona kwa nini kuzidi...  0.000000  ...  0.0   \n",
      "17  objetivo multiplicar factores num√©ricos mixtos...  0.000000  ...  0.0   \n",
      "\n",
      "    —Ç—Ä—è–±–≤–∞  —Ç—É–∫   —á–µ   —â–µ   ŸÅŸä   ‡™è‡™ï   ùë•ùë•   ùüèùüé   ùüèùüè  \n",
      "3      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "12     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "14     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "17     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[5 rows x 1009 columns]\n",
      "Preprocessed topics_df:\n",
      "                id                                             title  \\\n",
      "0   t_00004da3a1b2                        –æ—Ç–∫—Ä–∏–≤–∞–Ω–µ—Ç–æ –Ω–∞ —Ä–µ–∑–∏—Å—Ç–æ—Ä–∏—Ç–µ   \n",
      "2   t_00068291e9a4                   entradas e sa√≠das de uma fun√ß√£o   \n",
      "4   t_0006d41a73a8  –≥—Ä–∞—Ñ–∏–∫–∏ –Ω–∞ –µ–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª–Ω–∏ —Ñ—É–Ω–∫—Ü–∏–∏ –∞–ª–≥–µ–±—Ä–∞ 2 –Ω–∏–≤–æ   \n",
      "5   t_0008768bdee6                                               100   \n",
      "10  t_00102869fbcb                                  triangle polygon   \n",
      "\n",
      "                                          description channel      category  \\\n",
      "0   –∏–∑—Å–ª–µ–¥–≤–∞–Ω–µ –Ω–∞ –º–∞—Ç–µ—Ä–∏–∞–ª–∏ –∫–æ–∏—Ç–æ –ø—Ä–µ–¥–∏–∑–≤–∏–∫–≤–∞—Ç –Ω–∞–º...  000cf7        source   \n",
      "2                 entenda um pouco mais sobre fun√ß√µes  8e286a        source   \n",
      "4   –Ω–∞—É—á–∏ –ø–æ–≤–µ—á–µ –∑–∞ –≥—Ä–∞—Ñ–∏–∫–∏—Ç–µ –Ω–∞ —Å–ª–æ–∂–Ω–∏—Ç–µ –ø–æ–∫–∞–∑–∞—Ç–µ...  000cf7        source   \n",
      "5                                               37 49  5223e0  supplemental   \n",
      "10  learning outcome student must able solve probl...  a91e32       aligned   \n",
      "\n",
      "    level language          parent has_content  \\\n",
      "0     4.0       bg  t_16e29365b50d        True   \n",
      "2     4.0       pt  t_d14b6c2a2b70        True   \n",
      "4     4.0       bg  t_e2452e21d252        True   \n",
      "5     4.0       gu  t_0da7a331d666        True   \n",
      "10    3.0       en  t_039cecc12bb8        True   \n",
      "\n",
      "                                        combined_text  ...  —Ç–æ–π  —Ç—Ä—è–±–≤–∞  —Ç—É–∫  \\\n",
      "0   –æ—Ç–∫—Ä–∏–≤–∞–Ω–µ—Ç–æ –Ω–∞ —Ä–µ–∑–∏—Å—Ç–æ—Ä–∏—Ç–µ –∏–∑—Å–ª–µ–¥–≤–∞–Ω–µ –Ω–∞ –º–∞—Ç–µ—Ä...  ...  0.0     0.0  0.0   \n",
      "2   entradas e sa√≠das de uma fun√ß√£o entenda um pou...  ...  0.0     0.0  0.0   \n",
      "4   –≥—Ä–∞—Ñ–∏–∫–∏ –Ω–∞ –µ–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª–Ω–∏ —Ñ—É–Ω–∫—Ü–∏–∏ –∞–ª–≥–µ–±—Ä–∞ 2 –Ω–∏...  ...  0.0     0.0  0.0   \n",
      "5                       100 37 49 5223e0 supplemental  ...  0.0     0.0  0.0   \n",
      "10  triangle polygon learning outcome student must...  ...  0.0     0.0  0.0   \n",
      "\n",
      "     —á–µ   —â–µ   ŸÅŸä   ‡™è‡™ï   ùë•ùë•   ùüèùüé   ùüèùüè  \n",
      "0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2   0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4   0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "5   0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "10  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[5 rows x 1010 columns]\n"
     ]
    }
   ],
   "source": [
    "# perform tfidf vectorization\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# NLTK RESOURCES\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "#DATA PATH\n",
    "content_df = pd.read_csv(r'C:\\Users\\nh013\\Desktop\\Learning Equality - Curriculum Recommendations\\content.csv')\n",
    "correlations_df = pd.read_csv(r'C:\\Users\\nh013\\Desktop\\Learning Equality - Curriculum Recommendations\\correlations.csv')\n",
    "topics_df = pd.read_csv(r'C:\\Users\\nh013\\Desktop\\Learning Equality - Curriculum Recommendations\\topics.csv')\n",
    "\n",
    "# IDENTIFY MISSING VALUES\n",
    "missing_values_content = content_df.isnull().sum()\n",
    "missing_values_correlations = correlations_df.isnull().sum()\n",
    "missing_values_topics = topics_df.isnull().sum()\n",
    "\n",
    "# DROP MISSING VALUES\n",
    "content_df.dropna(inplace=True)\n",
    "correlations_df.dropna(inplace=True)\n",
    "topics_df.dropna(inplace=True)\n",
    "\n",
    "# REMOVE DUPLICATE ROWS\n",
    "content_df.drop_duplicates(inplace=True)\n",
    "correlations_df.drop_duplicates(inplace=True)\n",
    "topics_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# NLP PREPROCESSING FUNCTION\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [token for token in tokens if token.isalnum()]\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        return \" \".join(tokens)\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "# NLP PREPROCESSING IN CONTENT_DF\n",
    "content_df['description'] = content_df['description'].apply(preprocess_text)\n",
    "content_df['text'] = content_df['text'].apply(preprocess_text)\n",
    "\n",
    "# NLP PREPROCESSING IN TOPICS_DF\n",
    "topics_df['title'] = topics_df['title'].apply(preprocess_text)\n",
    "topics_df['description'] = topics_df['description'].apply(preprocess_text)\n",
    "topics_df['channel'] = topics_df['channel'].apply(preprocess_text)\n",
    "topics_df['category'] = topics_df['category'].apply(preprocess_text)\n",
    "\n",
    "# COMBINE TEXT COLUMN FOR  TF-IDF vectorization\n",
    "content_df['combined_text'] = content_df['description'] + \" \" + content_df['text']\n",
    "topics_df['combined_text'] = topics_df['title'] + \" \" + topics_df['description'] + \" \" + topics_df['channel'] + \" \" + topics_df['category']\n",
    "\n",
    "# TF-IDF VECTORIZATION\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "tfidf_combined_content = tfidf_vectorizer.fit_transform(content_df['combined_text'])\n",
    "tfidf_combined_topics = tfidf_vectorizer.transform(topics_df['combined_text'])\n",
    "\n",
    "# CONVERT TF-IDF MATRICS TO DATAFRAME \n",
    "tfidf_combined_df_content = pd.DataFrame(tfidf_combined_content.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "tfidf_combined_df_topics = pd.DataFrame(tfidf_combined_topics.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# CONCATINATE TF-IDF MATRICS TO DATAFRAME\n",
    "content_df = pd.concat([content_df, tfidf_combined_df_content], axis=1)\n",
    "topics_df = pd.concat([topics_df, tfidf_combined_df_topics], axis=1)\n",
    "\n",
    "\n",
    "print(\"Preprocessed content_df:\")\n",
    "print(content_df.head())\n",
    "\n",
    "print(\"Preprocessed topics_df:\")\n",
    "print(topics_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313a5706",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7b2506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "773873c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content Train Shape: (57624, 14)\n",
      "Content Test Shape: (14407, 14)\n",
      "Topics Train Shape: (43189, 13)\n",
      "Topics Test Shape: (10798, 13)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# NLTK RESOURCES\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# DATA PATH \n",
    "content_df = pd.read_csv(r'C:\\Users\\nh013\\Desktop\\Learning Equality - Curriculum Recommendations\\content.csv')\n",
    "correlations_df = pd.read_csv(r'C:\\Users\\nh013\\Desktop\\Learning Equality - Curriculum Recommendations\\correlations.csv')\n",
    "topics_df = pd.read_csv(r'C:\\Users\\nh013\\Desktop\\Learning Equality - Curriculum Recommendations\\topics.csv')\n",
    "\n",
    "# IDENTIFY MISSING VALUES\n",
    "missing_values_content = content_df.isnull().sum()\n",
    "missing_values_correlations = correlations_df.isnull().sum()\n",
    "missing_values_topics = topics_df.isnull().sum()\n",
    "\n",
    "# DROP MISSING VALUES\n",
    "content_df.dropna(inplace=True)\n",
    "correlations_df.dropna(inplace=True)\n",
    "topics_df.dropna(inplace=True)\n",
    "\n",
    "# REMOVE DUPLICATE ROWS\n",
    "content_df.drop_duplicates(inplace=True)\n",
    "correlations_df.drop_duplicates(inplace=True)\n",
    "topics_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# NLP PREPROCESSING FUNCTION\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [token for token in tokens if token.isalnum()]\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        return \" \".join(tokens)\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "# NLP PREPROCESSING IN CONTENT_DF\n",
    "content_df['description'] = content_df['description'].apply(preprocess_text)\n",
    "content_df['text'] = content_df['text'].apply(preprocess_text)\n",
    "\n",
    "# NLP PREPROCESSING IN TOPICS_DF\n",
    "topics_df['title'] = topics_df['title'].apply(preprocess_text)\n",
    "topics_df['description'] = topics_df['description'].apply(preprocess_text)\n",
    "topics_df['channel'] = topics_df['channel'].apply(preprocess_text)\n",
    "topics_df['category'] = topics_df['category'].apply(preprocess_text)\n",
    "\n",
    "# COMBINE TEXT COLUMN FOR  TF-IDF vectorization\n",
    "content_df['combined_text'] = content_df['description'] + \" \" + content_df['text']\n",
    "topics_df['combined_text'] = topics_df['title'] + \" \" + topics_df['description'] + \" \" + topics_df['channel'] + \" \" + topics_df['category']\n",
    "\n",
    "# TF-IDF VECTORIZATION\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "tfidf_combined_content = tfidf_vectorizer.fit_transform(content_df['combined_text'])\n",
    "tfidf_combined_topics = tfidf_vectorizer.transform(topics_df['combined_text'])\n",
    "\n",
    "# CONVERT TF-IDF MATRICES TO DATAFRAME\n",
    "tfidf_combined_df_content = pd.DataFrame(tfidf_combined_content.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "tfidf_combined_df_topics = pd.DataFrame(tfidf_combined_topics.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# CONCATINATE TF-IDF MATRICES TO DATAFRAME \n",
    "content_df = pd.concat([content_df, tfidf_combined_df_content], axis=1)\n",
    "topics_df = pd.concat([topics_df, tfidf_combined_df_topics], axis=1)\n",
    "\n",
    "\n",
    "# SELECT COLUMN FOR CONTENT DATAFRAME \n",
    "content_columns = ['id', 'title', 'description', 'kind', 'text', 'language', 'copyright_holder', 'license']\n",
    "content_data = content_df[content_columns]\n",
    "\n",
    "# SELECT COLUMN FOR TOPICS DATAFRAME\n",
    "topics_columns = ['id', 'title', 'description', 'channel', 'category', 'level', 'language', 'parent', 'has_content']\n",
    "topics_data = topics_df[topics_columns]\n",
    "\n",
    "# SPLIT CONTENT DATA FOR TRAINING AND TESTING SET\n",
    "content_train, content_test = train_test_split(content_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# SPLIT TOPICS DATA FOR TRAINING AND TESTING SET\n",
    "topics_train, topics_test = train_test_split(topics_data, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "print(\"Content Train Shape:\", content_train.shape)\n",
    "print(\"Content Test Shape:\", content_test.shape)\n",
    "print(\"Topics Train Shape:\", topics_train.shape)\n",
    "print(\"Topics Test Shape:\", topics_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94366101",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6e1c99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48569e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "218adf57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1915/1915 [==============================] - 124s 62ms/step - loss: 0.1698 - accuracy: 0.9505\n",
      "Epoch 2/5\n",
      "1915/1915 [==============================] - 116s 61ms/step - loss: 0.1289 - accuracy: 0.9626\n",
      "Epoch 3/5\n",
      "1915/1915 [==============================] - 117s 61ms/step - loss: 0.1125 - accuracy: 0.9670\n",
      "Epoch 4/5\n",
      "1915/1915 [==============================] - 117s 61ms/step - loss: 0.1070 - accuracy: 0.9691\n",
      "Epoch 5/5\n",
      "1915/1915 [==============================] - 117s 61ms/step - loss: 0.1082 - accuracy: 0.9696\n",
      "479/479 [==============================] - 13s 26ms/step - loss: 0.0850 - accuracy: 0.9708\n",
      "Test Accuracy: 0.9708129167556763\n"
     ]
    }
   ],
   "source": [
    "#perfrom rnn model \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "#NLTK RESOURCES\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# DATA PATH\n",
    "content_df = pd.read_csv(r'C:\\Users\\nh013\\Desktop\\Learning Equality - Curriculum Recommendations\\content.csv')\n",
    "correlations_df = pd.read_csv(r'C:\\Users\\nh013\\Desktop\\Learning Equality - Curriculum Recommendations\\correlations.csv')\n",
    "topics_df = pd.read_csv(r'C:\\Users\\nh013\\Desktop\\Learning Equality - Curriculum Recommendations\\topics.csv')\n",
    "\n",
    "# IDENTIFY MISSING VALUES\n",
    "missing_values_content = content_df.isnull().sum()\n",
    "missing_values_correlations = correlations_df.isnull().sum()\n",
    "missing_values_topics = topics_df.isnull().sum()\n",
    "\n",
    "# DROP MISSING VALUES\n",
    "content_df.dropna(inplace=True)\n",
    "correlations_df.dropna(inplace=True)\n",
    "topics_df.dropna(inplace=True)\n",
    "\n",
    "# REMOVE DUPLICATE ROWS\n",
    "content_df.drop_duplicates(inplace=True)\n",
    "correlations_df.drop_duplicates(inplace=True)\n",
    "topics_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# NLP PREPROCESSING FUNCTION\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [token for token in tokens if token.isalnum()]\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        return \" \".join(tokens)\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "# NLP PREPROCESSING FOR CONTENT_DF\n",
    "content_df['description'] = content_df['description'].apply(preprocess_text)\n",
    "content_df['text'] = content_df['text'].apply(preprocess_text)\n",
    "\n",
    "# NLP PREPROCESSING FOR TOPICS_DF\n",
    "topics_df['title'] = topics_df['title'].apply(preprocess_text)\n",
    "topics_df['description'] = topics_df['description'].apply(preprocess_text)\n",
    "topics_df['channel'] = topics_df['channel'].apply(preprocess_text)\n",
    "topics_df['category'] = topics_df['category'].apply(preprocess_text)\n",
    "\n",
    "# COMBINE TEXT COLUMN FOR TF-IDF VECTORIZATION\n",
    "content_df['combined_text'] = content_df['description'] + \" \" + content_df['text']\n",
    "topics_df['combined_text'] = topics_df['title'] + \" \" + topics_df['description'] + \" \" + topics_df['channel'] + \" \" + topics_df['category']\n",
    "\n",
    "# REBUILD 'combined_text' COLUMN AFTER DROPPING MISSING  VALUES\n",
    "content_df['combined_text'] = content_df['combined_text'].apply(preprocess_text)\n",
    "topics_df['combined_text'] = topics_df['combined_text'].apply(preprocess_text)\n",
    "\n",
    "# COMBINE CONTENT AND TOPICS DATA \n",
    "combined_data = content_df['combined_text'].tolist() + topics_df['combined_text'].tolist()\n",
    "combined_labels = np.concatenate((np.zeros(len(content_df)), np.ones(len(topics_df))))\n",
    "\n",
    "# SPLIT  COMBINED DATA INTO TRAINING AND TESTING SET \n",
    "train_data, test_data, train_labels, test_labels = train_test_split(combined_data, combined_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# TOKENIZE AND PAD SEQUENCE\n",
    "tokenizer = Tokenizer(num_words=1000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_data)\n",
    "train_sequences = tokenizer.texts_to_sequences(train_data)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data)\n",
    "train_padded = pad_sequences(train_sequences, maxlen=100, padding='post', truncating='post')\n",
    "test_padded = pad_sequences(test_sequences, maxlen=100, padding='post', truncating='post')\n",
    "\n",
    "# BUILD RNN MODEL\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=1000, output_dim=32, input_length=100),\n",
    "    LSTM(64),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# TRAIN THE MODEL \n",
    "model.fit(train_padded, train_labels, epochs=5, batch_size=32)\n",
    "\n",
    "# EVALUATE THE MODEL\n",
    "test_loss, test_accuracy = model.evaluate(test_padded, test_labels)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed838a66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
